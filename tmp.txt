import os
import json
import csv
from abc import ABC, abstractmethod
from typing import Dict, Iterator, Callable, Optional, List

class BaseDataLoader(ABC):
    """æ”¯æŒå›è°ƒæœºåˆ¶å’Œæ ¼å¼å¯¼å‡ºçš„æ•°æ®åŠ è½½å™¨åŸºç±»"""
    
    def __init__(self, file_path: str, state_file: str = "loader_state.json"):
        """
        :param file_path: æ•°æ®æ–‡ä»¶è·¯å¾„
        :param state_file: æ–­ç‚¹çŠ¶æ€è®°å½•æ–‡ä»¶è·¯å¾„
        """
        self.file_path = file_path
        self.state_file = state_file
        self.state = self._load_state()
        self.callback_registry = {}  # å­˜å‚¨å›è°ƒå‡½æ•°çš„æ³¨å†Œè¡¨
        self.current_index = 0  # å½“å‰å¤„ç†ä½ç½®
        
    def register_callback(self, callback_name: str, callback_func: Callable[[int], None]):
        """æ³¨å†Œå›è°ƒå‡½æ•°[2,3](@ref)
        
        :param callback_name: å›è°ƒç±»å‹ ('item_processed'æˆ–'final_save')
        :param callback_func: å›è°ƒå‡½æ•°
        """
        self.callback_registry[callback_name] = callback_func
        
    def _load_state(self) -> Dict:
        """åŠ è½½æ–­ç‚¹ç»­ä¼ çŠ¶æ€"""
        if os.path.exists(self.state_file):
            try:
                with open(self.state_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                print(f"çŠ¶æ€æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
        return {}
    
    def _save_state(self, current_index: int):
        """ä¿å­˜å½“å‰å¤„ç†è¿›åº¦[1](@ref)"""
        self.state[self.file_path] = current_index
        try:
            with open(self.state_file, 'w', encoding='utf-8') as f:
                json.dump(self.state, f, indent=4)
        except Exception as e:
            print(f"çŠ¶æ€ä¿å­˜å¤±è´¥: {e}")
    
    def get_start_index(self) -> int:
        """è·å–å½“å‰æ–‡ä»¶çš„èµ·å§‹å¤„ç†ä½ç½®"""
        return self.state.get(self.file_path, 0)
    
    def process_data(self) -> Iterator[Dict]:
        """æ•°æ®å¤„ç†ä¸»æµç¨‹ï¼Œæ”¯æŒæ–­ç‚¹ç»­ä¼ å’Œå›è°ƒé€šçŸ¥[1,4](@ref)"""
        start_index = self.get_start_index()
        print(f"ä»ç´¢å¼• {start_index} å¼€å§‹å¤„ç†æ–‡ä»¶: {self.file_path}")
        self.current_index = start_index
        
        try:
            for i, item in enumerate(self._read_file()):
                if i < start_index:
                    continue
                
                # ç»Ÿä¸€è¾“å‡ºæ ¼å¼
                result = {
                    "raw_data": item,
                    "source": self.file_path,
                    "index": i
                }
                
                # ä¸åœ¨æ­¤å¤„æ›´æ–°çŠ¶æ€ï¼ç­‰å¾…å›è°ƒé€šçŸ¥
                self.current_index = i
                yield result
                
        except Exception as e:
            print(f"æ–‡ä»¶å¤„ç†ä¸­æ–­: {e}")
            raise
        finally:
            # æ‰§è¡Œæœ€ç»ˆçŠ¶æ€ä¿å­˜å›è°ƒ
            if "final_save" in self.callback_registry:
                self.callback_registry["final_save"](self.current_index)
                
            print(f"æ–‡ä»¶å¤„ç†å®Œæˆ: {self.file_path}, æœ€åå¤„ç†ç´¢å¼•: {self.current_index}")

    def notify_item_processed(self, item_index: int):
        """é€šçŸ¥åŠ è½½å™¨æŸä¸ªæ•°æ®é¡¹å·²å¤„ç†å®Œæˆ[2,3](@ref)
        
        :param item_index: å·²å¤„ç†é¡¹çš„ç´¢å¼•
        """
        # æ›´æ–°çŠ¶æ€ä¸ºä¸‹ä¸€ä¸ªç´¢å¼•
        self._save_state(item_index + 1)
        
        # æ‰§è¡Œå·²æ³¨å†Œçš„å›è°ƒå‡½æ•°
        if "item_processed" in self.callback_registry:
            self.callback_registry["item_processed"](item_index)
    
    def dump_to_format(
        self, 
        output_path: str, 
        format_type: str = "alpaca", 
        field_mapping: Optional[Dict[str, str]] = None
    ):
        """
        å°†åŠ è½½çš„æ•°æ®å¯¼å‡ºä¸ºAlpacaæˆ–ShareGPTæ ¼å¼[6,7,8](@ref)
        
        :param output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        :param format_type: å¯¼å‡ºæ ¼å¼ ('alpaca' æˆ– 'sharegpt')
        :param field_mapping: å­—æ®µæ˜ å°„å…³ç³»ï¼Œç”¨äºè‡ªå®šä¹‰å­—æ®µåç§°
        """
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        # è®¾ç½®é»˜è®¤å­—æ®µæ˜ å°„
        default_mapping = {
            "instruction": "instruction",
            "input": "input",
            "output": "output",
            "system": "system",
            "history": "history"
        }
        field_map = field_mapping or default_mapping
        
        with open(output_path, 'w', encoding='utf-8') as f:
            # å¤„ç†æ¯æ¡æ•°æ®
            for data_item in self.process_data():
                raw = data_item["raw_data"]
                
                if format_type == "alpaca":
                    # æ„å»ºAlpacaæ ¼å¼æ•°æ®[8](@ref)
                    formatted_item = {
                        "instruction": raw.get(field_map["instruction"], ""),
                        "input": raw.get(field_map["input"], ""),
                        "output": raw.get(field_map["output"], "")
                    }
                    
                    # æ·»åŠ å¯é€‰å­—æ®µ
                    if "system" in field_map:
                        formatted_item["system"] = raw.get(field_map["system"], "")
                    if "history" in field_map:
                        formatted_item["history"] = raw.get(field_map["history"], [])
                    
                    # å†™å…¥JSONè¡Œæ ¼å¼
                    f.write(json.dumps(formatted_item, ensure_ascii=False) + "\n")
                    
                elif format_type == "sharegpt":
                    # æ„å»ºShareGPTæ ¼å¼æ•°æ®[6,7](@ref)
                    conversations = []
                    if "conversations" in raw:
                        # å¦‚æœå·²æœ‰å¯¹è¯ç»“æ„ï¼Œç›´æ¥ä½¿ç”¨
                        conversations = raw["conversations"]
                    else:
                        # å¦åˆ™ä»æŒ‡ä»¤å’Œè¾“å‡ºæ„å»ºå¯¹è¯
                        conversations = [
                            {"from": "human", "value": raw.get(field_map["instruction"], "")},
                            {"from": "gpt", "value": raw.get(field_map["output"], "")}
                        ]
                    
                    formatted_item = {
                        "conversations": conversations
                    }
                    
                    # æ·»åŠ å¯é€‰å­—æ®µ
                    if "system" in field_map:
                        formatted_item["system"] = raw.get(field_map["system"], "")
                    if "tools" in raw:
                        formatted_item["tools"] = raw.get("tools", "")
                    
                    # å†™å…¥JSONè¡Œæ ¼å¼
                    f.write(json.dumps(formatted_item, ensure_ascii=False) + "\n")
                
                # é€šçŸ¥è¯¥é¡¹å·²å¤„ç†
                self.notify_item_processed(data_item["index"])
        
        print(f"æˆåŠŸå¯¼å‡ºæ•°æ®åˆ° {output_path} ({format_type.upper()}æ ¼å¼)")

    @abstractmethod
    def _read_file(self) -> Iterator[Dict]:
        """å­ç±»éœ€å®ç°çš„å…·ä½“æ–‡ä»¶è¯»å–æ–¹æ³•"""
        pass


class JsonDataLoader(BaseDataLoader):
    """JSONæ ¼å¼æ•°æ®åŠ è½½å™¨ï¼Œæ”¯æŒå®Œæ•´JSONæ•°ç»„å’ŒJSONLæ ¼å¼"""
    
    def _read_file(self) -> Iterator[Dict]:
        """è¯»å–JSONæ–‡ä»¶ï¼ˆæ”¯æŒjsonlinesæ ¼å¼ï¼‰"""
        with open(self.file_path, 'r', encoding='utf-8') as f:
            # å°è¯•è§£æä¸ºjsonlinesï¼ˆæ¯è¡Œä¸€ä¸ªJSONå¯¹è±¡ï¼‰
            for i, line in enumerate(f):
                try:
                    if line.strip():  # è·³è¿‡ç©ºè¡Œ
                        yield json.loads(line.strip())
                except json.JSONDecodeError:
                    print(f"ç¬¬ {i} è¡ŒJSONè§£æå¤±è´¥ï¼Œè·³è¿‡")
            
            # å¦‚æœä¸æ˜¯jsonlinesæ ¼å¼ï¼Œå°è¯•è§£æä¸ºå®Œæ•´JSONæ•°ç»„
            f.seek(0)
            try:
                data = json.load(f)
                if isinstance(data, list):
                    for item in data:
                        yield item
            except json.JSONDecodeError:
                raise ValueError("æ–‡ä»¶ä¸æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼")


class TsvDataLoader(BaseDataLoader):
    """TSVæ ¼å¼æ•°æ®åŠ è½½å™¨"""
    
    def _read_file(self) -> Iterator[Dict]:
        """è¯»å–TSVæ–‡ä»¶"""
        with open(self.file_path, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f, delimiter='\t')
            for row in reader:
                yield row











# åˆå§‹åŒ–JSONåŠ è½½å™¨
loader = JsonDataLoader("original_data.json", "loader_state.json")

# æ³¨å†Œå›è°ƒå‡½æ•°
def on_item_processed(index):
    print(f"âœ… å·²å¤„ç†ç´¢å¼•: {index}")
    
def on_final_save(last_index):
    print(f"ğŸ’¾ æœ€ç»ˆçŠ¶æ€ä¿å­˜: {last_index}")
    # å¯æ·»åŠ æ¸…ç†èµ„æºç­‰æ“ä½œ

loader.register_callback("item_processed", on_item_processed)
loader.register_callback("final_save", on_final_save)

# å¤„ç†æ•°æ®
for data_item in loader.process_data():
    # å®é™…å¤„ç†é€»è¾‘ï¼ˆå¦‚è°ƒç”¨LLM APIï¼‰
    processed_data = process_with_llm(data_item)
    save_result(processed_data)
    
    # å¿…é¡»è°ƒç”¨é€šçŸ¥
    loader.notify_item_processed(data_item["index"])









# åŠ è½½JSONæ•°æ®
loader = JsonDataLoader("original_data.json")

# è‡ªå®šä¹‰å­—æ®µæ˜ å°„
custom_mapping = {
    "instruction": "query",
    "output": "response",
    "system": "role"
}

# å¯¼å‡ºä¸ºAlpacaæ ¼å¼ï¼ˆè‡ªåŠ¨æ³¨å†Œå›è°ƒï¼‰
loader.dump_to_format(
    "output/alpaca_data.jsonl", 
    "alpaca", 
    field_mapping=custom_mapping
)






# åŠ è½½TSVæ•°æ®
tsv_loader = TsvDataLoader("conversations.tsv")

# å¯¼å‡ºä¸ºShareGPTæ ¼å¼
tsv_loader.dump_to_format(
    "output/sharegpt_data.jsonl", 
    "sharegpt"
)







# åˆå§‹åŒ–åŠ è½½å™¨ï¼ˆè‡ªåŠ¨è¯»å–çŠ¶æ€æ–‡ä»¶ï¼‰
loader = JsonDataLoader("large_dataset.json", "loader_state.json")

try:
    for data_item in loader.process_data():
        # æ¨¡æ‹Ÿå¤„ç†è¿‡ç¨‹ä¸­æ–­
        if random.random() < 0.01:
            raise Exception("æ¨¡æ‹Ÿå¤„ç†ä¸­æ–­")
        
        # å®é™…å¤„ç†é€»è¾‘
        process_item(data_item)
        
        # é€šçŸ¥å®Œæˆ
        loader.notify_item_processed(data_item["index"])
except Exception as e:
    print(f"å¤„ç†ä¸­æ–­: {e}")
    
# é‡æ–°è¿è¡Œå°†ä»ä¸Šæ¬¡ä½ç½®ç»§ç»­
