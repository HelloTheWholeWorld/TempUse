import os
import json
import csv
from abc import ABC, abstractmethod
from typing import Dict, Iterator, List, Union, Optional
from datetime import datetime

class BaseDataLoader(ABC):
    """数据加载器基类，支持断点续传功能"""
    
    def __init__(self, file_path: str, state_file: str = "loader_state.json"):
        self.file_path = file_path
        self.state_file = state_file
        self.state = self._load_state()
        
    def _load_state(self) -> Dict:
        """加载断点续传状态"""
        if os.path.exists(self.state_file):
            try:
                with open(self.state_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                print(f"状态文件加载失败: {e}")
        return {}
    
    def _save_state(self, current_index: int):
        """保存当前处理进度"""
        self.state[self.file_path] = current_index
        try:
            with open(self.state_file, 'w', encoding='utf-8') as f:
                json.dump(self.state, f, indent=4)
        except Exception as e:
            print(f"状态保存失败: {e}")
    
    def get_start_index(self) -> int:
        """获取当前文件的起始处理位置"""
        return self.state.get(self.file_path, 0)
    
    def process_data(self) -> Iterator[Dict]:
        """数据处理主流程，支持断点续传"""
        start_index = self.get_start_index()
        print(f"从索引 {start_index} 开始处理文件: {self.file_path}")
        
        try:
            for i, item in enumerate(self._read_file()):
                if i < start_index:
                    continue
                
                result = {
                    "raw_data": item,
                    "source": self.file_path,
                    "index": i
                }
                
                self._save_state(i + 1)
                yield result
                
        except Exception as e:
            print(f"文件处理中断: {e}")
            raise
        finally:
            print(f"文件处理完成: {self.file_path}, 最后处理索引: {self.get_start_index()}")

    @abstractmethod
    def _read_file(self) -> Iterator[Dict]:
        pass

    # 新增的Alpaca/ShareGPT导出功能
    def dump_to_format(
        self, 
        output_path: str, 
        format_type: str = "alpaca", 
        field_mapping: Optional[Dict[str, str]] = None
    ):
        """
        将加载的数据导出为Alpaca或ShareGPT格式[2,3](@ref)
        
        :param output_path: 输出文件路径
        :param format_type: 导出格式 ('alpaca' 或 'sharegpt')
        :param field_mapping: 字段映射关系，用于自定义字段名称
        """
        data = list(self.process_data())
        formatted_data = []
        
        # 设置默认字段映射
        default_mapping = {
            "instruction": "instruction",
            "input": "input",
            "output": "output",
            "system": "system",
            "history": "history"
        }
        field_map = field_mapping or default_mapping
        
        for item in data:
            raw = item["raw_data"]
            
            if format_type == "alpaca":
                # 构建Alpaca格式数据[1,2](@ref)
                formatted_item = {
                    "instruction": raw.get(field_map["instruction"], ""),
                    "input": raw.get(field_map["input"], ""),
                    "output": raw.get(field_map["output"], ""),
                    "system": raw.get(field_map["system"], ""),
                    "history": raw.get(field_map["history"], [])
                }
                formatted_data.append(formatted_item)
                
            elif format_type == "sharegpt":
                # 构建ShareGPT格式数据[2,3](@ref)
                conversations = []
                if "conversations" in raw:
                    # 如果已有对话结构，直接使用
                    conversations = raw["conversations"]
                else:
                    # 否则从指令和输出构建对话
                    conversations = [
                        {"from": "human", "value": raw.get(field_map["instruction"], "")},
                        {"from": "gpt", "value": raw.get(field_map["output"], "")}
                    ]
                
                formatted_item = {
                    "conversations": conversations,
                    "system": raw.get(field_map["system"], ""),
                    "tools": raw.get("tools", "")  # ShareGPT特有字段
                }
                formatted_data.append(formatted_item)
        
        # 创建输出目录
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        # 写入文件
        with open(output_path, 'w', encoding='utf-8') as f:
            if format_type == "alpaca":
                json.dump(formatted_data, f, ensure_ascii=False, indent=2)
            else:
                # ShareGPT格式使用JSONL
                for item in formatted_data:
                    f.write(json.dumps(item, ensure_ascii=False) + "\n")
        
        print(f"成功导出 {len(formatted_data)} 条数据到 {output_path} ({format_type.upper()}格式)")


class JsonDataLoader(BaseDataLoader):
    """JSON格式数据加载器，支持完整JSON数组和JSONL格式[1,6](@ref)"""
    
    def _read_file(self) -> Iterator[Dict]:
        """读取JSON文件（支持jsonlines格式）"""
        with open(self.file_path, 'r', encoding='utf-8') as f:
            # 尝试解析为jsonlines（每行一个JSON对象）
            for i, line in enumerate(f):
                try:
                    yield json.loads(line.strip())
                except json.JSONDecodeError:
                    print(f"第 {i} 行JSON解析失败，跳过")
            
            # 如果不是jsonlines格式，尝试解析为完整JSON数组
            f.seek(0)
            try:
                data = json.load(f)
                if isinstance(data, list):
                    for item in data:
                        yield item
            except json.JSONDecodeError:
                raise ValueError("文件不是有效的JSON格式")


class TsvDataLoader(BaseDataLoader):
    """TSV格式数据加载器[9,10](@ref)"""
    
    def _read_file(self) -> Iterator[Dict]:
        """读取TSV文件"""
        with open(self.file_path, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f, delimiter='\t')
            for row in reader:
                yield row















import os
import json
import csv
from abc import ABC, abstractmethod
from typing import Dict, Iterator, Callable, Optional

class BaseDataLoader(ABC):
    """支持回调机制的数据加载器基类"""
    
    def __init__(self, file_path: str, state_file: str = "loader_state.json"):
        self.file_path = file_path
        self.state_file = state_file
        self.state = self._load_state()
        self.callback_registry = {}  # 存储回调函数的注册表
        
    def register_callback(self, callback_name: str, callback_func: Callable[[int], None]):
        """注册回调函数"""
        self.callback_registry[callback_name] = callback_func
        
    def _load_state(self) -> Dict:
        """加载断点续传状态"""
        if os.path.exists(self.state_file):
            try:
                with open(self.state_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                print(f"状态文件加载失败: {e}")
        return {}
    
    def _save_state(self, current_index: int):
        """保存当前处理进度"""
        self.state[self.file_path] = current_index
        try:
            with open(self.state_file, 'w', encoding='utf-8') as f:
                json.dump(self.state, f, indent=4)
        except Exception as e:
            print(f"状态保存失败: {e}")
    
    def get_start_index(self) -> int:
        """获取当前文件的起始处理位置"""
        return self.state.get(self.file_path, 0)
    
    def process_data(self) -> Iterator[Dict]:
        """数据处理主流程，支持断点续传和回调通知"""
        start_index = self.get_start_index()
        print(f"从索引 {start_index} 开始处理文件: {self.file_path}")
        
        try:
            for i, item in enumerate(self._read_file()):
                if i < start_index:
                    continue
                
                # 统一输出格式
                result = {
                    "raw_data": item,
                    "source": self.file_path,
                    "index": i
                }
                
                # 不在此处更新状态！等待回调通知
                yield result
                
        except Exception as e:
            print(f"文件处理中断: {e}")
            raise
        finally:
            # 执行最终状态保存回调
            if "final_save" in self.callback_registry:
                self.callback_registry["final_save"](self.get_start_index())
                
            print(f"文件处理完成: {self.file_path}, 最后处理索引: {self.get_start_index()}")

    def notify_item_processed(self, item_index: int):
        """通知加载器某个数据项已处理完成"""
        # 更新状态为下一个索引
        self._save_state(item_index + 1)
        
        # 执行已注册的回调函数
        if "item_processed" in self.callback_registry:
            self.callback_registry["item_processed"](item_index)

    @abstractmethod
    def _read_file(self) -> Iterator[Dict]:
        pass

class JsonDataLoader(BaseDataLoader):
    """JSON格式数据加载器"""
    
    def _read_file(self) -> Iterator[Dict]:
        """读取JSON文件（支持jsonlines格式）"""
        with open(self.file_path, 'r', encoding='utf-8') as f:
            # 尝试解析为jsonlines（每行一个JSON对象）
            for i, line in enumerate(f):
                try:
                    yield json.loads(line.strip())
                except json.JSONDecodeError:
                    print(f"第 {i} 行JSON解析失败，跳过")
            
            # 如果不是jsonlines格式，尝试解析为完整JSON数组
            f.seek(0)
            try:
                data = json.load(f)
                if isinstance(data, list):
                    for item in data:
                        yield item
            except json.JSONDecodeError:
                raise ValueError("文件不是有效的JSON格式")
