import asyncio
import aiohttp
import json
import logging
import os
from tenacity import (
    retry,
    stop_after_attempt,
    wait_exponential,
    retry_if_exception_type
)
from typing import Dict, Any, List, Optional, Tuple, Union
# from config import CONFIG

import dashscope
from dashscope import Generation
from http import HTTPStatus
from concurrent.futures import ThreadPoolExecutor
import threading

# 配置日志
# logging_level = logging.DEBUG if CONFIG['DEBUG'] else logging.INFO
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger("ModelAPIEngine")


class ModelAPIClient:
    # 类级别的线程锁和线程池
    _thread_pool = ThreadPoolExecutor(max_workers=10)  # 可根据需要调整线程数
    _qwen_global_lock = threading.Lock()
    """统一多模型API客户端，支持云端API和本地Ollama"""
    def __init__(
        self,
        model_type: str = "openai",
        api_key: Optional[str] = None,
        base_url: Optional[str] = None,
        model_name: str = "gpt-4-turbo",
        max_retries: int = 3,
        timeout: int = 60,
        max_concurrent: int = 10,
        save_results: bool = False,
        output_dir: str = "results",
        **kwargs
    ):
        """
        初始化API客户端
        :param model_type: 模型类型 (openai/deepseek/ollama/qwen/vllm)
        :param api_key: API密钥（云端API需要）
        :param base_url: 自定义基础URL（可选）
        :param model_name: 模型名称
        :param max_retries: 最大重试次数
        :param timeout: 请求超时时间（秒）
        :param max_concurrent: 最大并发请求数
        :param save_results: 是否保存结果文件
        :param output_dir: 结果保存目录
        :param kwargs: 模型特定参数，如temperature, top_p等
        """
        self.model_type = model_type.lower()
        self.api_key = api_key
        self.model_name = model_name
        self.max_retries = max_retries
        self.timeout = timeout
        self.semaphore = asyncio.Semaphore(max_concurrent)
        self.model_kwargs = kwargs  # 存储模型特定参数
        self.save_results = save_results
        self.output_dir = output_dir
        if 'is_qwen' in kwargs:
            self.is_qwen = kwargs['is_qwen']
        else:
            self.is_qwen = False
        
        # 确保输出目录存在
        if save_results:
            os.makedirs(self.output_dir, exist_ok=True)
            logger.info(f"结果将保存到目录: {os.path.abspath(self.output_dir)}")
        
        # 设置API基础URL
        self.base_url = self._resolve_base_url(base_url)
        
        # 设置特定API端点
        self.endpoints = {
            "openai": "/v1/chat/completions",
            "deepseek": "/v1/chat/completions",
            "ollama": "/api/generate",
            "qwen": "/api/v1/services/aigc/text-generation/generation",
            "vllm": "/v1/chat/completions"
        }

        # 模型特定参数模板
        self.payload_templates = {
            "openai": {
                "model": model_name,
                "messages": [{"role": "user", "content": ""}],
                "temperature": 0.7,
                "top_p": 1.0,
                "max_tokens": None,
                "stream": False
            },
            "deepseek": {
                "model": model_name,
                "messages": [{"role": "user", "content": ""}],
                "temperature": 0.7,
                "top_p": 1.0,
                "max_tokens": None,
                "stream": False
            },
            "qwen": {
                "model": model_name,
                "input": {
                    "messages": [
                        {"role": "system", "content": ""},
                        {"role": "user", "content": ""}
                    ]
                },
                "parameters": {
                    "result_format": "message",
                    "temperature": 0.7,
                    "top_p": 1.0,
                    "enable_search": False,
                    "stream": False,
                    "enable_thinking": False,
                    "incremental_output": False\
                }
            },
            "ollama": {
                "model": model_name,
                "prompt": "",
                "stream": False,
                "options": {
                    "temperature": 0.7,
                    "top_p": 1.0,
                    "num_ctx": 2048
                }
            },
            "vllm": {
                "model": model_name,
                "messages": [{"role": "user", "content": ""}],
                "temperature": 0.7,
                "top_p": 1.0,
                "max_tokens": None,
                "stream": False,
                "chat_template_kwargs": {},  # 用于传递特殊参数"
                "extra_body": {}  # 用于传递特殊参数
            }
        }
        logger.info(f"Initialized {model_type.upper()} client for model: {model_name}")

    def _resolve_base_url(self, base_url: Optional[str]) -> str:
        """解析基础URL"""
        if base_url:
            return base_url.rstrip('/')
        
        # 默认URL配置
        defaults = {
            "openai": "https://api.openai.com",
            "deepseek": "https://api.deepseek.com",
            "ollama": "http://localhost:11434",
            "qwen": "https://dashscope.aliyuncs.com",
            "vllm": "http://127.0.0.1:9160"
        }
        return defaults.get(self.model_type, "")
    
    def _build_payload(self, prompt: str) -> Dict[str, Any]:
        """构建模型特定的请求负载"""
        if self.model_type not in self.payload_templates:
            raise ValueError(f"Unsupported model type: {self.model_type}")
        
        payload = self.payload_templates[self.model_type].copy()

        # 更新payload中的模型特定参数
        if self.model_type == "ollama":
            payload["prompt"] = prompt
            # 更新Ollama的options参数
            for key in ["temperature", "top_p", "num_ctx",]:
                if key in self.model_kwargs:
                    payload["options"][key] = self.model_kwargs[key]
            if "enable_thinking" in self.model_kwargs and not self.model_kwargs['enable_thinking']:
                payload["prompt"] = '/no_think' + prompt
        elif self.model_type == "qwen":
            # 更新Qwen的消息内容
            payload["input"]["messages"][1]["content"] = prompt

            # 自动启用流式输出当启用思考模式
            if self.model_kwargs.get("enable_thinking", False):
                payload["parameters"]["stream"] = True
                payload["parameters"]["incremental_output"] = True
            
            # 更新参数
            for key in ["temperature", "top_p", "enable_search", "stream", 
                        "enable_thinking", "incremental_output", "thinking_budget"]:
                if key in self.model_kwargs:
                    payload["parameters"][key] = self.model_kwargs[key]
                    
        elif self.model_type == "vllm":
                payload["messages"][0]["content"] = prompt
                # 设置Qwen思考模式参数
                if "enable_thinking" in self.model_kwargs:
                    payload["chat_template_kwargs"] = {
                        "enable_thinking": self.model_kwargs["enable_thinking"]
                    }
                # 更新其他参数
                for key in ["temperature", "top_p", "max_tokens", "stream"]:
                    if key in self.model_kwargs:
                        payload[key] = self.model_kwargs[key]
        else:
            payload["messages"][0]["content"] = prompt
            # 更新通用参数
            for key in ["temperature", "top_p", "max_tokens", "stream"]:
                if key in self.model_kwargs:
                    payload[key] = self.model_kwargs[key]
        return payload
    
    def _build_headers(self) -> Dict[str, str]:
        """构建请求头，包含认证信息"""
        headers = {"Content-Type": "application/json"}
        
        if self.model_type == "openai" and self.api_key:
            headers["Authorization"] = f"Bearer {self.api_key}"
        elif self.model_type == "deepseek" and self.api_key:
            headers["Authorization"] = f"Bearer {self.api_key}"
        elif self.model_type == "qwen" and self.api_key:
            # 添加官方要求的额外头部
            headers["Authorization"] = f"Bearer {self.api_key}"
            # headers["X-DashScope-Async"] = "disable"  # 确保同步调用
        # Ollama不需要认证头
        return headers
    
    def _extract_response(self, response_data: Dict) -> str:
        """从不同模型的响应中提取文本内容"""
        if self.model_type == "ollama":
            return response_data.get("response", "")
        elif self.model_type in ["openai", "deepseek"]:
            return response_data["choices"][0]["message"]["content"]
        elif self.model_type == "qwen":
            # 根据官方格式提取响应内容, 这里只是非思考模式
            return response_data["output"]["choices"][0]["message"]["content"]
        elif self.model_type == "vllm":
            reasoning_content = ""
            if self.is_qwen and response_data["choices"][0]["message"]["reasoning_content"]:
                reasoning_content = response_data["choices"][0]["message"]["reasoning_content"]
                reasoning_content = '<think>\n' + reasoning_content + '\n</think>\n\n'
            return reasoning_content + response_data["choices"][0]["message"]["content"]
        else:
            return json.dumps(response_data)

    def _handle_qwen_stream_sync(self, prompt: str) -> str:
        """使用dashscope SDK处理Qwen思考模式的流式响应(同步版本)"""
        reasoning_content = ""
        final_content = ""
        
        # 构建extra_body参数
        extra_body = {"enable_thinking": True}
        if "thinking_budget" in self.model_kwargs:
            extra_body["thinking_budget"] = self.model_kwargs["thinking_budget"]
        if "enable_search" in self.model_kwargs:
            extra_body["enable_search"] = self.model_kwargs["enable_search"]

        # 使用线程锁保护全局API密钥设置
        with ModelAPIClient._qwen_global_lock:
            original_api_key = dashscope.api_key
            dashscope.api_key = self.api_key
        
        try:
            # 调用DashScope API
            responses = Generation.call(
                model=self.model_name,
                messages=[{"role": "user", "content": prompt}],
                stream=True,
                incremental_output=True,
                extra_body=extra_body,
                result_format="message"
            )
            
            for r in responses:
                if r.status_code != HTTPStatus.OK:
                    raise Exception(f"API错误: {r.code} - {r.message}")
                
                # 解析响应
                if r.output and r.output.choices:
                    choice = r.output.choices[0]
                    message = choice.message
                    
                    # 获取思考内容
                    if hasattr(message, 'reasoning_content') and message.reasoning_content:
                        reasoning_content += message.reasoning_content
                    
                    # 获取回答内容
                    if hasattr(message, 'content') and message.content:
                        final_content += message.content
        finally:
            # 恢复原始API密钥
            with ModelAPIClient._qwen_global_lock:
                dashscope.api_key = original_api_key
        
        return '<think>\n' + reasoning_content + '\n</think>\n\n' + final_content

    async def _handle_qwen_non_thinking_stream(self, response: aiohttp.ClientResponse) -> str:
        """处理Qwen非思考模式的流式响应"""
        full_content = ""
        async for line in response.content:
            if line:
                decoded_line = line.decode('utf-8').strip()
                if decoded_line.startswith('data:'):
                    try:
                        event_data = json.loads(decoded_line[5:])
                        content = event_data.get('output', {}).get('choices', [{}])[0].get('message', {}).get('content', '')
                        if content:
                            full_content += content
                    except json.JSONDecodeError:
                        logger.warning(f"JSON解析失败: {decoded_line}")
        return full_content
    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=2, max=30),
        retry=retry_if_exception_type((Exception,)),
        before_sleep=lambda retry_state: logger.warning(
            f"Retry #{retry_state.attempt_number} for request due to {retry_state.outcome.exception()}"
        )
    )
    async def _make_async_request(self, session: aiohttp.ClientSession, prompt: str) -> str:
        """执行异步API请求（带重试机制）"""
        # 对于启用了思考模式的Qwen请求，使用dashscope SDK
        if self.model_type == "qwen" and self.model_kwargs.get("enable_thinking", False):
            try:
                loop = asyncio.get_running_loop()
                # 使用线程池执行同步阻塞调用
                result = await loop.run_in_executor(
                    ModelAPIClient._thread_pool, 
                    self._handle_qwen_stream_sync, 
                    prompt
                )
                return result
            except Exception as e:
                logger.error(f"Qwen思考模式调用失败: {str(e)}")
                raise
        
        # 其他模型和非思考模式的Qwen请求保持原实现
        url = f"{self.base_url}{self.endpoints[self.model_type]}"
        payload = self._build_payload(prompt)
        headers = self._build_headers()

        logger.debug(f"API请求: {url}")
        logger.debug(f"API请求参数: {payload}")
        logger.debug(f"API请求头: {headers}")

        async with self.semaphore:
            try:
                async with session.post(
                    url,
                    json=payload,
                    headers=headers,
                    timeout=self.timeout
                    ) as response:
                    if response.status >= 400:
                        try:
                            error_data = await response.json()
                            print(error_data)
                        except Exception:
                            error_text = await response.text()
                            print(error_text)
                        # print(f"API错误: {response.status} - {response.reason}")
                    # logging.debug(response)
                    response.raise_for_status()
                    
                    # 处理Qwen非思考模式的流式响应
                    if self.model_type == "qwen" and payload["parameters"].get("stream", False):
                        return await self._handle_qwen_non_thinking_stream(response)
                    
                    # 非流式响应处理
                    response_data = await response.json()
                    logger.debug(f"API响应: {response_data}")
                    return self._extract_response(response_data)
                    
            except Exception as e:
                logger.error(f"API请求失败: {str(e)}")
                raise

    async def generate(self, session: aiohttp.ClientSession, prompt: str) -> Union[str, Dict[str, str]]:
        """
        生成文本内容
        
        :param session: aiohttp会话对象
        :param prompt: 输入的提示文本
        :return: 模型生成的文本（或包含思考过程的字典）
        """
        try:
            result = await self._make_async_request(session, prompt)
            return result
        except Exception as e:
            logger.error(f"所有重试失败: {prompt[:50]}... 错误: {str(e)}")
            return ""

class AsyncBatchProcessor:
    """异步批量处理器"""
    
    def __init__(self, api_clients: List[ModelAPIClient]):
        """
        初始化批量处理器
        
        :param api_clients: 配置好的API客户端列表
        """
        self.api_clients = api_clients
        self.session = None
        logger.info(f"Initialized batch processor with {len(api_clients)} API clients")
    
    async def __aenter__(self):
        self.session = aiohttp.ClientSession()
        return self
    
    async def __aexit__(self, exc_type, exc, tb):
        await self.session.close()

    async def _generate_and_record(self, session: aiohttp.ClientSession, client: ModelAPIClient, prompt: str) -> Tuple[Union[str, Dict[str, str]], str]:
        """调用生成方法并记录模型名称"""
        try:
            result = await client.generate(session, prompt)
            return (result, client.model_name)
        except Exception as e:
            logger.error(f"生成失败: {prompt[:50]}... 错误: {str(e)}")
            return ("", client.model_name)
    
    async def process_batch(self, prompts: List[str]) -> List[Tuple[Union[str, Dict[str, str]], str]]:
        """
        处理一批提示文本，返回结果和对应的模型名称
        
        :param prompts: 提示文本列表
        :return: 列表，每个元素是元组 (生成内容, 模型名称)
        """
        if not self.session:
            raise RuntimeError("Use async context manager (async with)")
        print('11111111111111111111111111111111111')
        for p in prompts:
            print(p)
        
        # 轮询使用不同的API客户端
        client_cycle = self._client_cycle()
        
        tasks = []
        for prompt in prompts:
            client = next(client_cycle)
            # 创建任务时保存当前客户端信息
            task = asyncio.create_task(self._generate_and_record(self.session, client, prompt))
            tasks.append(task)
            logger.debug(f"Submitted task for prompt: {prompt[:30]}...")
        
        # 等待所有任务完成
        results = await asyncio.gather(*tasks, return_exceptions=True)
        for res in results:
            print(res)
        
        # 处理异常结果
        final_results = []
        for res in results:
            if isinstance(res, Exception):
                logger.error(f"Task failed: {str(res)}")
                # 异常时返回空结果和模型名称占位符
                final_results.append(("", ""))
            else:
                final_results.append(res)
        
        return final_results
    
    def _client_cycle(self):
        """无限循环提供API客户端"""
        while True:
            for client in self.api_clients:
                yield client

def get_qwen_api_think_client():
    qwen_client = ModelAPIClient(
        model_type="qwen",
        api_key="sk-426c613869b34e508a78e0153e3198e6",
        model_name="qwen3-4b",
        max_concurrent=3,
        max_retries=1,
        save_results=True,
        output_dir="./qwen_results",
        enable_thinking=True,
        thinking_budget=4000,
        stream=True,  # 必须启用流式以支持思考模式
        incremental_output=True  # 启用增量输出
    )
    return qwen_client

def get_qwen_api_no_think_client():
    qwen_client = ModelAPIClient(
        model_type="qwen",
        api_key="sk-426c613869b34e508a78e0153e3198e6",
        model_name="qwen3-4b",
        max_concurrent=3,
        max_retries=1,
        save_results=True,
        output_dir="./qwen_results",
        enable_thinking=False,
        stream=False
    )
    return qwen_client

def get_vllm_qwen_think_client():
    qwen_client = ModelAPIClient(
        model_type="vllm",
        api_key="",
        model_name="/mnt/data/Llmei/data/Qwen/Qwen3-32B",
        max_concurrent=3,
        max_retries=1,
        save_results=False,
        max_tokens=4096,
        output_dir="./qwen_results",
        enable_thinking=True,
        stream=False,
        is_qwen=True,
    )
    return qwen_client

def get_vllm_qwen_no_think_client():
    qwen_client = ModelAPIClient(
        model_type="vllm",
        api_key="",
        model_name="/home/pyhou/projects/models/Qwen3-8B",
        max_concurrent=3,
        max_retries=1,
        save_results=False,
        output_dir="./qwen_results",
        max_tokens=4096,
        enable_thinking=False,
        stream=False,
        is_qwen=True,
    )
    return qwen_client

# 使用示例
async def main():
    # 创建Qwen客户端（启用思维链模式）
    # think_qwen_client = get_qwen_api_think_client()
    # no_think_qwen_client = get_qwen_api_no_think_client()
    
    # vllm client
    think_vllm_qwen_client = get_vllm_qwen_think_client()
    no_think_vllm_qwen_client = get_vllm_qwen_no_think_client()

    # 初始化批量处理器
    async with AsyncBatchProcessor(
        api_clients=[no_think_vllm_qwen_client]
    ) as processor:
        prompts = [
            "解释量子计算的基本原理",
            "计算：如果一辆车在2小时内行驶了120公里，那么它的平均速度是多少公里/小时？",
            "动物园有15只狮子和20只老虎。如果将3只狮子转移到另一个动物园，还剩下多少只大型猫科动物？",
            '讲一下诸葛亮',
            '讲一下李四光',
            '讲一下邓稼先',
            '讲一下北京市',
            '讲一下天津市',
            '讲一下杭州市',
        ]

        results = await processor.process_batch(prompts)
        
        # 输出结果
        for i, (prompt, (result, model_name)) in enumerate(zip(prompts, results)):
            print(f"提示 {i+1}: {prompt}")
            print(f"模型: {model_name}")
            print(f"回答: {str(result)[:300]}...")
            print("-" * 50)

if __name__ == "__main__":
    asyncio.run(main())


# DEBUG
# Right
# curl -X POST "http://127.0.0.1:9160/v1/completions" -H "Content-Type: application/json" -d '{"model": "/mnt/data/Llmei/data/Qwen/Qwen3-8B", "prompt": "解释量子计算的基本原理", "max_tokens": 1024, "temperature": 0.0, "top_p": 1.0, "stream": false ,"extra_body": {"chat_template_kwargs": {"enable_thinking": false}} }'
# curl -X POST "http://127.0.0.1:9160/v1/chat/completions" -H "Content-Type: application/json" -d '{"model": "/mnt/data/Llmei/data/Qwen/Qwen3-8B", "messages": [{"role": "user", "content": "解释量子计算的基本原理"}], "max_tokens": 1024, "temperature": 0.0, "top_p": 1.0, "stream": false ,"extra_body": {"chat_template_kwargs": {"enable_thinking": false}} }'
# curl -X POST "http://127.0.0.1:9160/v1/chat/completions" -H "Content-Type: application/json" -d '{"model": "/mnt/data/Llmei/data/Qwen/Qwen3-8B", "messages": [{"role": "user", "content": "解释量子计算的基本原理"}], "max_tokens": 1024, "temperature": 0.0, "top_p": 1.0, "stream": false , "chat_template_kwargs": {"enable_thinking": false} }'


# Fault
# curl -X POST "http://127.0.0.1:9160/v1/completions" -H "Content-Type: application/json" -d '{"model": "/mnt/data/Llmei/data/Qwen/Qwen3-32B-MLX-bf16", "messages": [{"role": "user", "content": "解释量子计算的基本原理"}], "max_tokens": 1024, "temperature": 0.0, "top_p": 1.0, "stream": false ,"extra_body": {"chat_template_kwargs": {"enable_thinking": false}} }'
