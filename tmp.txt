import os
from evalscope import TaskConfig, run_task
from evalscope.constants import JudgeStrategy

task_cfg = TaskConfig(
    model='/mnt/data/Llmei/data/Qwen/Qwen3-8B',
    api_url='http://127.0.0.1:9160/v1/chat/completions',
    eval_type='service',
    datasets=[
        'general_qa',
    ],
    dataset_args={
        'general_qa': {
            'local_path': '/mnt/data/Llmei/data/eval/space_cal84.jsonl',
        }
    },
    eval_batch_size=128,
    generation_config={
        'max_tokens': 16384,  # 最大生成token数，建议设置为较大值避免输出截断
        'temperature': 0.6,  # 采样温度 (qwen 报告推荐值)
        'top_p': 0.95,  # top-p采样 (qwen 报告推荐值)
        'top_k': 20,  # top-k采样 (qwen 报告推荐值)
        'n': 1,  # 每个请求产生的回复数量
    },
    timeout=60000,  # 超时时间
    stream=False,  # 是否使用流式输出
    # limit=1000,  # 设置为100条数据进行测试


    # judge 相关参数
    judge_model_args={
        'model_id': '/mnt/data/Llmei/data/Qwen/Qwen3-8B',
        'api_url': 'http://127.0.0.1:9160/v1/chat/completions',
        'api_key': "",
        'generation_config': {
            'temperature': 0.0,
            'max_tokens': 4096,
        },
        # 根据参考答案和模型输出，判断模型输出是否正确
        'score_type': 'pattern',
    },
    # judge 并发数
    judge_worker_num=1,
    # 使用 LLM 进行评测
    judge_strategy=JudgeStrategy.LLM,
    # analysis_report=True,
)

run_task(task_cfg=task_cfg)

